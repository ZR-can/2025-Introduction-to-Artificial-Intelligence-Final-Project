@article{BOWLING2002215,
  title = {Multiagent Learning Using a Variable Learning Rate},
  author = {Bowling, Michael and Veloso, Manuela},
  year = {2002},
  journal = {Artificial Intelligence},
  volume = {136},
  number = {2},
  pages = {215--250},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(02)00121-2},
  abstract = {Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents' policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, ``Win or Learn Fast'', for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method.},
  keywords = {Game theory,Multiagent learning,Reinforcement learning}
}

@article{busoniuComprehensiveSurveyMultiagent2008a,
  title = {A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}},
  author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  year = {2008},
  month = mar,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {38},
  number = {2},
  pages = {156--172},
  issn = {1558-2442},
  doi = {10.1109/TSMCC.2007.913919},
  urldate = {2025-09-26},
  abstract = {Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.},
  keywords = {Control systems,Distributed control,Environmental economics,Feedback,game theory,Learning,Marine technology,Mechanical engineering,multiagent systems,Multiagent systems,reinforcement learning,Resource management,Robots},
  file = {C:\Users\ZR\Zotero\storage\GFTQQQHS\Busoniu 等 - 2008 - A Comprehensive Survey of Multiagent Reinforcement Learning.pdf}
}

@incollection{busoniuMultiagentReinforcementLearning2010,
  title = {Multi-Agent {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Multi-Agent {{Reinforcement Learning}}},
  booktitle = {Innovations in {{Multi-Agent Systems}} and {{Applications}} - 1},
  author = {Bu{\c s}oniu, Lucian and Babu{\v s}ka, Robert and De Schutter, Bart},
  editor = {Srinivasan, Dipti and Jain, Lakhmi C.},
  year = {2010},
  pages = {183--221},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-14435-6_7},
  urldate = {2025-09-23},
  abstract = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of multi-agent reinforcement learning algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of multi-agent reinforcement learning are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where multi-agent reinforcement learning techniques have been applied are briefly discussed. Several multi-agent reinforcement learning algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the multi-agent reinforcement learning field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
  isbn = {978-3-642-14435-6},
  langid = {english},
  keywords = {Markov Decision Process,Multiagent System,Nash Equilibrium,Reinforcement Learning,Reward Function},
  file = {C:\Users\ZR\Zotero\storage\QRLT46HL\Buşoniu 等 - 2010 - Multi-agent Reinforcement Learning An Overview.pdf}
}

@article{clausDynamicsReinforcementLearning,
  title = {The {{Dynamics}} of {{Reinforcement Learning}} in {{Cooperative Multiagent Systems}}},
  author = {Claus, Caroline and Boutilier, Craig},
  abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of partial action observability, game structure, and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria and on learned Qvalues.},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\3YAGEVNP\Claus和Boutilier - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf}
}

@misc{conitzerAWESOMEGeneralMultiagent2003,
  title = {{{AWESOME}}: {{A General Multiagent Learning Algorithm}} That {{Converges}} in {{Self-Play}} and {{Learns}} a {{Best Response Against Stationary Opponents}}},
  shorttitle = {{{AWESOME}}},
  author = {Conitzer, Vincent and Sandholm, Tuomas},
  year = {2003},
  month = jul,
  number = {arXiv:cs/0307002},
  eprint = {cs/0307002},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cs/0307002},
  urldate = {2025-09-27},
  abstract = {A satisfactory multiagent learning algorithm should, \{{\textbackslash}em at a minimum\}, learn to play optimally against stationary opponents and converge to a Nash equilibrium in self-play. The algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action repeated games--assuming that the opponent's (mixed) strategy is observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have these two properties in \{{\textbackslash}em all\} repeated (finite) games. It requires only that the other players' actual actions (not their strategies) can be observed at each step. It also learns to play optimally against opponents that \{{\textbackslash}em eventually become\} stationary. The basic idea behind AWESOME (\{{\textbackslash}em Adapt When Everybody is Stationary, Otherwise Move to Equilibrium\}) is to try to adapt to the others' strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing other multiagent learning algorithms also.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\Q6W3RQKP\\Conitzer和Sandholm - 2003 - AWESOME A General Multiagent Learning Algorithm that Converges in Self-Play and Learns a Best Respo.pdf;C\:\\Users\\ZR\\Zotero\\storage\\4A94CXLQ\\0307002.html}
}

@article{critesElevatorGroupControl1998,
  title = {Elevator {{Group Control Using Multiple Reinforcement Learning Agents}}},
  author = {Crites, Robert H. and Barto, Andrew G.},
  year = {1998},
  month = nov,
  journal = {Machine Learning},
  volume = {33},
  number = {2-3},
  pages = {235--262},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1007518724497},
  urldate = {2025-09-24},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\36T4EZ5B\Crites和Barto - 1998 - Elevator Group Control Using Multiple Reinforcement Learning Agents.pdf}
}

@article{critesElevatorGroupControl1998a,
  title = {Elevator {{Group Control Using Multiple Reinforcement Learning Agents}}},
  author = {Crites, Robert H. and Barto, Andrew G.},
  year = {1998},
  month = nov,
  journal = {Machine Learning},
  volume = {33},
  number = {2},
  pages = {235--262},
  issn = {1573-0565},
  doi = {10.1023/A:1007518724497},
  abstract = {Recent algorithmic and theoretical advances in reinforcement learning (RL) have attracted widespread interest. RL algorithms have appeared that approximate dynamic programming on an incremental basis. They can be trained on the basis of real or simulated experiences, focusing their computation on areas of state space that are actually visited during control, making them computationally tractable on very large problems. If each member of a team of agents employs one of these algorithms, a new collective learning algorithm emerges for the team as a whole. In this paper we demonstrate that such collective RL algorithms can be powerful heuristic methods for addressing large-scale control problems.}
}

@book{fudenbergTheoryLearningGames1998,
  title = {The {{Theory}} of {{Learning}} in {{Games}}},
  author = {Fudenberg, Drew and Levine, David K.},
  editor = {Binmore, Ken},
  year = {1998},
  month = jun,
  series = {Economic {{Learning}} and {{Social Evolution}}},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  isbn = {978-0-262-52924-2},
  langid = {english}
}

@article{guoJointOptimizationHandover2020,
  title = {Joint {{Optimization}} of {{Handover Control}} and {{Power Allocation Based}} on {{Multi-Agent Deep Reinforcement Learning}}},
  author = {Guo, Delin and Tang, Lan and Zhang, Xinggan and Liang, Ying-Chang},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {69},
  number = {11},
  pages = {13124--13138},
  issn = {1939-9359},
  doi = {10.1109/TVT.2020.3020400},
  urldate = {2025-09-11},
  abstract = {In this paper, we study the handover (HO), and power allocation problem in a two-tier heterogeneous network (HetNet), which consists of a macro base station, and some millimeter-wave (mmWave) small base stations. We establish an HO management, and power allocation scheme to maximize the overall throughput while reducing the HO frequency. In particular, considering the interrelationship among decisions made by different user equipments (UEs), we first model the HO, and power allocation problem as a fully cooperative multi-agent task, in which all agents, i.e., UEs, have the same target. Then, to solve the multi-agent task, and get decentralized policies for each UE, we develop a multi-agent reinforcement learning (MARL) algorithm based on the proximal policy optimization (PPO) method, by introducing the centralized training with decentralized execution framework. That is, we use global information to train policies for each UE, and after the training is finished, each UE obtains a decentralized policy, which can be implemented only based on each UE's local observation. Specially, we introduce the counterfactual baseline to address the credit assignment problem in centralized learning. Due to the centralized training, the decentralized polices learned by multi-agent PPO (MAPPO) can work more cooperatively. Finally, the simulation results demonstrate that our method can achieve better performance comparing with other existing works.},
  langid = {american},
  keywords = {Base stations,Handover,HetNet,mmWave,multi-agent deep reinforcement learning,Optimization,power allocation,Resource management,Task analysis,Throughput},
  file = {C:\Users\ZR\Zotero\storage\AMHIWSR6\Guo 等 - 2020 - Joint Optimization of Handover Control and Power Allocation Based on Multi-Agent Deep Reinforcement.pdf}
}

@misc{hongRethinkingIndividualGlobal2022,
  title = {Rethinking {{Individual Global Max}} in {{Cooperative Multi-Agent Reinforcement Learning}}},
  author = {Hong, Yitian and Jin, Yaochu and Tang, Yang},
  year = {2022},
  month = sep,
  number = {arXiv:2209.09640},
  eprint = {2209.09640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.09640},
  urldate = {2025-09-27},
  abstract = {In cooperative multi-agent reinforcement learning, centralized training and decentralized execution (CTDE) has achieved remarkable success. Individual Global Max (IGM) decomposition, which is an important element of CTDE, measures the consistency between local and joint policies. The majority of IGM-based research focuses on how to establish this consistent relationship, but little attention has been paid to examining IGM's potential flaws. In this work, we reveal that the IGM condition is a lossy decomposition, and the error of lossy decomposition will accumulated in hypernetwork-based methods. To address the above issue, we propose to adopt an imitation learning strategy to separate the lossy decomposition from Bellman iterations, thereby avoiding error accumulation. The proposed strategy is theoretically proved and empirically verified on the StarCraft Multi-Agent Challenge benchmark problem with zero sight view. The results also confirm that the proposed method outperforms state-of-the-art IGM-based approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Multiagent Systems},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\6UGSP4RF\\Hong 等 - 2022 - Rethinking Individual Global Max in Cooperative Multi-Agent Reinforcement Learning.pdf;C\:\\Users\\ZR\\Zotero\\storage\\NHCAUV7E\\2209.html}
}

@article{kapetanakisReinforcementLearningCoordination,
  title = {Reinforcement {{Learning}} of {{Coordination}} in {{Cooperative Multi-Agent Systems}}},
  author = {Kapetanakis, Spiros and Kudenko, Daniel},
  abstract = {We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multiagent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible.},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\IG5Z5BTK\Kapetanakis和Kudenko - Reinforcement Learning of Coordination in Cooperative Multi-Agent Systems.pdf}
}

@inproceedings{lauerAlgorithmDistributedReinforcement2000,
  title = {An {{Algorithm}} for {{Distributed Reinforcement Learning}} in {{Cooperative Multi-Agent Systems}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Lauer, M. and Riedmiller, Martin A.},
  year = {2000},
  month = jun,
  urldate = {2025-09-27},
  abstract = {Semantic Scholar extracted view of "An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems" by M. Lauer et al.}
}

@inproceedings{liCelebratingDiversityShared2021,
  title = {Celebrating {{Diversity}} in {{Shared Multi-Agent Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Chenghao and Wang, Tonghan and Wu, Chengjie and Zhao, Qianchuan and Yang, Jun and Zhang, Chongjie},
  year = {2021},
  volume = {34},
  pages = {3991--4002},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-09-27},
  abstract = {Recently, deep multi-agent reinforcement learning (MARL) has shown the promise to solve complex cooperative tasks. Its success is partly because of parameter sharing among agents. However, such sharing may lead agents to behave similarly and limit their coordination capacity. In this paper, we aim to introduce diversity in both optimization and representation of shared multi-agent reinforcement learning. Specifically, we propose an information-theoretical regularization to maximize the mutual information between agents' identities and their trajectories, encouraging extensive exploration and diverse individualized behaviors. In representation, we incorporate agent-specific modules in the shared neural network architecture, which are regularized by L1-norm to promote learning sharing among agents while keeping necessary diversity. Empirical results show that our method achieves state-of-the-art performance on Google Research Football and super hard StarCraft II micromanagement tasks.},
  file = {C:\Users\ZR\Zotero\storage\C4437YFW\Li 等 - 2021 - Celebrating Diversity in Shared Multi-Agent Reinforcement Learning.pdf}
}

@article{LITTMAN200155,
  title = {Value-Function Reinforcement Learning in {{Markov}} Games},
  author = {Littman, Michael L.},
  year = {2001},
  journal = {Cognitive Systems Research},
  volume = {2},
  number = {1},
  pages = {55--66},
  issn = {1389-0417},
  doi = {10.1016/S1389-0417(01)00015-8},
  abstract = {Markov games are a model of multiagent environments that are convenient for studying multiagent reinforcement learning. This paper describes a set of reinforcement-learning algorithms based on estimating value functions and presents convergence theorems for these algorithms. The main contribution of this paper is that it presents the convergence theorems in a way that makes it easy to reason about the behavior of simultaneous learners in a shared environment.},
  keywords = {-learning,Game theory,Markov games,Nash equilibria,Reinforcement learning,Temporal difference learning,Value functions}
}

@incollection{littmanMarkovGamesFramework1994,
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Littman, Michael L.},
  year = {1994},
  pages = {157--163},
  publisher = {Elsevier},
  doi = {10.1016/B978-1-55860-335-6.50027-1},
  urldate = {2025-09-27},
  abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  isbn = {978-1-55860-335-6},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\SVKH7T7P\Littman - 1994 - Markov games as a framework for multi-agent reinforcement learning.pdf}
}

@misc{liuXuanCeComprehensiveUnified2023,
  title = {{{XuanCe}}: {{A Comprehensive}} and {{Unified Deep Reinforcement Learning Library}}},
  shorttitle = {{{XuanCe}}},
  author = {Liu, Wenzhang and Cai, Wenzhe and Jiang, Kun and Cheng, Guangran and Wang, Yuanda and Wang, Jiawei and Cao, Jingyu and Xu, Lele and Mu, Chaoxu and Sun, Changyin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.16248},
  eprint = {2312.16248},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.16248},
  urldate = {2025-09-26},
  abstract = {In this paper, we present XuanCe, a comprehensive and unified deep reinforcement learning (DRL) library designed to be compatible with PyTorch, TensorFlow, and MindSpore. XuanCe offers a wide range of functionalities, including over 40 classical DRL and multi-agent DRL algorithms, with the flexibility to easily incorporate new algorithms and environments. It is a versatile DRL library that supports CPU, GPU, and Ascend, and can be executed on various operating systems such as Ubuntu, Windows, MacOS, and EulerOS. Extensive benchmarks conducted on popular environments including MuJoCo, Atari, and StarCraftII multi-agent challenge demonstrate the library's impressive performance. XuanCe is open-source and can be accessed at https://github.com/agi-brain/xuance.git.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Digital Libraries,Computer Science - Machine Learning},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\JQ6RZEB2\\Liu 等 - 2023 - XuanCe A Comprehensive and Unified Deep Reinforcement Learning Library.pdf;C\:\\Users\\ZR\\Zotero\\storage\\7URGUNPR\\2312.html}
}

@misc{loweMultiAgentActorCriticMixed2020,
  title = {Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}},
  author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  year = {2020},
  month = mar,
  number = {arXiv:1706.02275},
  eprint = {1706.02275},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.02275},
  urldate = {2025-09-27},
  abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\QIC7ILEM\\Lowe 等 - 2020 - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf;C\:\\Users\\ZR\\Zotero\\storage\\DYBPRN88\\1706.html}
}

@article{messingIntroductionMultiAgentSystems2002,
  title = {An {{Introduction}} to {{MultiAgent Systems}}},
  author = {Messing, Barbara},
  year = {2002},
  month = jun,
  journal = {K{\"u}nstliche Intell.},
  urldate = {2025-09-24},
  abstract = {The study of multi-agent systems (MAS) focuses on systems in which many intelligent agents interact with each other. These agents are considered to be autonomous entities such as software programs or robots. Their interactions can either be cooperative (for example as in an ant colony) or selfish (as in a free market economy). This book assumes only basic knowledge of algorithms and discrete maths, both of which are taught as standard in the first or second year of computer science degree programmes. A basic knowledge of artificial intelligence would useful to help understand some of the issues, but is not essential. The books main aims are: To introduce the student to the concept of agents and multi-agent systems, and the main applications for which they are appropriate To introduce the main issues surrounding the design of intelligent agents To introduce the main issues surrounding the design of a multi-agent society To introduce a number of typical applications for agent technology}
}

@inproceedings{NIPS2002_f8e59f4b,
  title = {Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wang, Xiaofeng and Sandholm, Tuomas},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2002},
  volume = {15},
  publisher = {MIT Press}
}

@inproceedings{NIPS2003_e71e5cd1,
  title = {Extending Q-Learning to General Adaptive Multi-Agent Systems},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Tesauro, Gerald},
  editor = {Thrun, S. and Saul, L. and Sch{\"o}lkopf, B.},
  year = {2003},
  volume = {16},
  publisher = {MIT Press}
}

@book{oliehoekConciseIntroductionDecentralized2016,
  title = {A {{Concise Introduction}} to {{Decentralized POMDPs}}},
  author = {Oliehoek, Frans A. and Amato, Christopher},
  year = {2016},
  series = {{{SpringerBriefs}} in {{Intelligent Systems}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-28929-8},
  urldate = {2025-09-27},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-319-28927-4 978-3-319-28929-8},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\P5CI4HAZ\Oliehoek和Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf}
}

@misc{openaiDota2Large2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k e}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  number = {arXiv:1912.06680},
  eprint = {1912.06680},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.06680},
  urldate = {2025-09-24},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ZR\Zotero\storage\RMBSIYGK\OpenAI 等 - 2019 - Dota 2 with Large Scale Deep Reinforcement Learning.pdf}
}

@inproceedings{powersNewCriteriaNew2004,
  title = {New {{Criteria}} and a {{New Algorithm}} for {{Learning}} in {{Multi-Agent Systems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Powers, Rob and Shoham, Yoav},
  year = {2004},
  volume = {17},
  publisher = {MIT Press},
  urldate = {2025-09-27},
  abstract = {We propose a new set of criteria for learning algorithms in multi-agent          systems, one that is more stringent and (we argue) better justified than          previous proposed criteria. Our criteria, which apply most straightfor-          wardly in repeated games with average rewards, consist of three require-          ments: (a) against a specified class of opponents (this class is a parameter          of the criterion) the algorithm yield a payoff that approaches the payoff          of the best response, (b) against other opponents the algorithm's payoff          at least approach (and possibly exceed) the security level payoff (or max-          imin value), and (c) subject to these requirements, the algorithm achieve          a close to optimal payoff in self-play. We furthermore require that these          average payoffs be achieved quickly. We then present a novel algorithm,          and show that it meets these new criteria for a particular parameter class,          the class of stationary opponents. Finally, we show that the algorithm          is effective not only in theory, but also empirically. Using a recently          introduced comprehensive game theoretic test suite, we show that the          algorithm almost universally outperforms previous learning algorithms.},
  file = {C:\Users\ZR\Zotero\storage\ENRFBUTH\Powers和Shoham - 2004 - New Criteria and a New Algorithm for Learning in Multi-Agent Systems.pdf}
}

@misc{rashidQMIXMonotonicValue2018,
  title = {{{QMIX}}: {{Monotonic Value Function Factorisation}} for {{Deep Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{QMIX}}},
  author = {Rashid, Tabish and Samvelyan, Mikayel and de Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  year = {2018},
  month = jun,
  number = {arXiv:1803.11485},
  eprint = {1803.11485},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.11485},
  urldate = {2025-09-11},
  abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {C:\Users\ZR\Zotero\storage\ZJIGM77S\Rashid 等 - 2018 - QMIX Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.pdf}
}

@inproceedings{rashidWeightedQMIXExpanding2020,
  title = {Weighted {{QMIX}}: {{Expanding Monotonic Value Function Factorisation}} for {{Deep Multi-Agent Reinforcement Learning}}},
  shorttitle = {Weighted {{QMIX}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rashid, Tabish and Farquhar, Gregory and Peng, Bei and Whiteson, Shimon},
  year = {2020},
  volume = {33},
  pages = {10199--10210},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-09-26},
  file = {C:\Users\ZR\Zotero\storage\6XM2ATKZ\Rashid 等 - 2020 - Weighted QMIX Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement L.pdf}
}

@inbook{riedmillerReinforcementLearningCooperating2001,
  title = {Reinforcement {{Learning}} for {{Cooperating}} and {{Communicating Reactive Agents}} in {{Electrical Power Grids}}},
  booktitle = {Balancing {{Reactivity}} and {{Social Deliberation}} in {{Multi-Agent Systems}}},
  author = {Riedmiller, Martin and Moore, Andrew and Schneider, Jeff},
  year = {2001},
  volume = {2103},
  pages = {137--149},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44568-4_9},
  urldate = {2025-09-24},
  collaborator = {Hannebauer, Markus and Wendler, Jan and Pagello, Enrico},
  isbn = {978-3-540-42327-0 978-3-540-44568-5},
  file = {C:\Users\ZR\Zotero\storage\WMZDCCIM\Riedmiller 等 - 2001 - Reinforcement Learning for Cooperating and Communicating Reactive Agents in Electrical Power Grids.pdf}
}

@misc{robertsonReviewIntroductionMultiAgent2004,
  type = {Text.{{Article}}},
  title = {Review of {{An Introduction}} to {{Multi-Agent Systems}}},
  author = {Robertson, Duncan A.},
  year = {2004},
  month = jun,
  publisher = {{Journal of Artificial Societies and Social Simulation}},
  urldate = {2025-09-24},
  abstract = {Review of:Wooldridge, Michael (2002) An Introduction to Multi-Agent Systems. John Wiley and Sons Limited: Chichester},
  copyright = {https://www.jasss.org},
  howpublished = {https://www.jasss.org/7/3/reviews/robertson.html},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\JT7UUT9M\robertson.html}
}

@misc{samvelyanStarCraftMultiAgentChallenge2019,
  title = {The {{StarCraft Multi-Agent Challenge}}},
  author = {Samvelyan, Mikayel and Rashid, Tabish and de Witt, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
  year = {2019},
  month = dec,
  number = {arXiv:1902.04043},
  eprint = {1902.04043},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.04043},
  urldate = {2025-09-28},
  abstract = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ\_obZ0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\8PI98BUB\\Samvelyan 等 - 2019 - The StarCraft Multi-Agent Challenge.pdf;C\:\\Users\\ZR\\Zotero\\storage\\HZLCXFFU\\1902.html}
}

@inproceedings{steingroverReinforcementLearningTraffic2005,
  title = {Reinforcement {{Learning}} of {{Traffic Light Controllers Adapting}} to {{Traffic Congestion}}.},
  booktitle = {{{BNAIC}}},
  author = {Steingrover, Merlijn and Schouten, Roelant and Peelen, Stefan and Nijhuis, Emil and Bakker, Bram},
  year = {2005},
  pages = {216--223},
  urldate = {2025-09-24},
  file = {C:\Users\ZR\Zotero\storage\NN8HB36K\Steingrover 等 - 2005 - Reinforcement Learning of Traffic Light Controllers Adapting to Traffic Congestion..pdf}
}

@article{stoneMultiagentSystemsSurvey2000,
  title = {Multiagent {{Systems}}: {{A Survey}} from a {{Machine Learning Perspective}}},
  shorttitle = {Multiagent {{Systems}}},
  author = {Stone, Peter and Veloso, Manuela},
  year = {2000},
  month = jun,
  journal = {Auton. Robots},
  volume = {8},
  number = {3},
  pages = {345--383},
  issn = {0929-5593},
  doi = {10.1023/A:1008942012299},
  urldate = {2025-09-26},
  abstract = {Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.}
}

@misc{sunehagValueDecompositionNetworksCooperative2017,
  title = {Value-{{Decomposition Networks For Cooperative Multi-Agent Learning}}},
  author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
  year = {2017},
  month = jun,
  number = {arXiv:1706.05296},
  eprint = {1706.05296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.05296},
  urldate = {2025-09-27},
  abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent" problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\RT5Y8GWG\\Sunehag 等 - 2017 - Value-Decomposition Networks For Cooperative Multi-Agent Learning.pdf;C\:\\Users\\ZR\\Zotero\\storage\\MLLYL7ZX\\1706.html}
}

@article{tesauroPricingAgentEconomies2002,
  title = {Pricing in {{Agent Economies Using Multi-Agent Q-Learning}}},
  author = {Tesauro, Gerald and Kephart, Jeffrey O.},
  year = {2002},
  month = sep,
  journal = {Autonomous Agents and Multi-Agent Systems},
  volume = {5},
  number = {3},
  pages = {289--304},
  issn = {1387-2532, 1573-7454},
  doi = {10.1023/A:1015504423309},
  urldate = {2025-09-24},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  langid = {english}
}

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko},
  year = {2019},
  journal = {nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {Nature Publishing Group},
  urldate = {2025-09-24},
  file = {C:\Users\ZR\Zotero\storage\KQCXPQPD\Vinyals 等 - 2019 - Grandmaster level in StarCraft II using multi-agent reinforcement learning.pdf}
}

@book{weissMultiagentSystems2016,
  title = {Multiagent {{Systems}}},
  editor = {Weiss, Gerhard and Arkin, Ronald C. and Christensen, Associate Editors: Henrik and Durfee, Edmund and Laschi, Cecilia and Murphy, Robin R. and Wooldridge, Michael},
  year = {2016},
  month = oct,
  series = {Intelligent {{Robotics}} and {{Autonomous Agents}} Series},
  edition = {2},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {The new edition of an introduction to multiagent systems that captures the state of the art in both theory and practice, suitable as textbook or reference.},
  isbn = {978-0-262-53387-4},
  langid = {english}
}

@misc{wittIndependentLearningAll2020,
  title = {Is {{Independent Learning All You Need}} in the {{StarCraft Multi-Agent Challenge}}?},
  author = {de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip H. S. and Sun, Mingfei and Whiteson, Shimon},
  year = {2020},
  month = nov,
  number = {arXiv:2011.09533},
  eprint = {2011.09533},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.09533},
  urldate = {2025-09-27},
  abstract = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the {\textbackslash}emph\{centralized training with decentralized execution\} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning. We also compare IPPO to several variants; the results suggest that IPPO's strong performance may be due to its robustness to some forms of environment non-stationarity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\7SNPQRYX\\Witt 等 - 2020 - Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge.pdf;C\:\\Users\\ZR\\Zotero\\storage\\R673LL5M\\2011.html}
}

@inproceedings{yuSurprisingEffectivenessPPO2022,
  title = {The {{Surprising Effectiveness}} of {{PPO}} in {{Cooperative Multi-Agent Games}}},
  booktitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  year = {2022},
  month = jun,
  urldate = {2025-09-11},
  abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, the Hanabi challenge, and Google Research Football, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods are a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at https://github.com/marlbenchmark/on-policy.},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\WBHJKNGY\Yu 等 - 2022 - The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games.pdf}
}

@misc{zhangMultiAgentReinforcementLearning2021,
  title = {Multi-{{Agent Reinforcement Learning}}: {{A Selective Overview}} of {{Theories}} and {{Algorithms}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  author = {Zhang, Kaiqing and Yang, Zhuoran and Ba{\c s}ar, Tamer},
  year = {2021},
  month = apr,
  number = {arXiv:1911.10635},
  eprint = {1911.10635},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.10635},
  urldate = {2025-09-26},
  abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {C\:\\Users\\ZR\\Zotero\\storage\\CQ8YB5DD\\Zhang 等 - 2021 - Multi-Agent Reinforcement Learning A Selective Overview of Theories and Algorithms.pdf;C\:\\Users\\ZR\\Zotero\\storage\\8XXAY6VI\\1911.html}
}

@article{zinkevichOnlineConvexProgramming,
  title = {Online {{Convex Programming}} and {{Generalized Infinitesimal Gradient Ascent}}},
  author = {Zinkevich, Martin},
  abstract = {Convex programming involves a convex set F {$\subseteq$} Rn and a convex cost function c : F {$\rightarrow$} R. The goal of convex programming is to find a point in F which minimizes c. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain. We also apply this algorithm to repeated games, and show that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent.},
  langid = {english},
  file = {C:\Users\ZR\Zotero\storage\8DWRFH3D\Zinkevich - Online Convex Programming and Generalized Infinitesimal Gradient Ascent.pdf}
}
