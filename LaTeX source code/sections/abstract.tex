% =============================================
% ==         sections/abstract.tex           ==
% =============================================

\begin{abstract}

MAPPO has emerged as a leading on-policy algorithm in Multi-Agent Reinforcement Learning (MARL), 
achieving state-of-the-art (SOTA) results across numerous benchmarks. 
Its Centralized Training, Decentralized Execution (CTDE) framework has demonstrated 
significant potential for addressing key MARL challenges in domains such as robotic collaboration and autonomous driving. 
In this work, we conduct a systematic benchmark of MAPPO against a suite of classic MARL algorithms, 
namely QMIX, OW-QMIX, and the Independent PPO (IPPO) baseline. 
Leveraging the StarCraft~II environment within the XuanCe library, 
we evaluate these algorithms across diverse multi-agent scenarios. 
Our comparative analysis focuses on key metrics, including win rate, convergence speed, 
training stability, and hyperparameter sensitivity. 
The objective is to elucidate MAPPO's performance advantages, define its operational boundaries, 
and understand the underlying reasons for its effectiveness in various cooperative contexts. 
Our results indicate that MAPPO substantially outperforms its counterparts, 
particularly in tasks demanding complex spatial coordination and collaboration among heterogeneous agents. 
Finally, this study offers practical guidance for the algorithm's selection in real-world applications. 
The implementation is available at: \urlstyle{tt}\url{https://github.com/ZR-can/2025-Introduction-to-Artificial-Intelligence-Final-Project}.

\end{abstract}

\begin{IEEEkeywords}
multi-agent reinforcement learning, MAPPO, CTDE, StarCraft~II,comparative analysis.
\end{IEEEkeywords}